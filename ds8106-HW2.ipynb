{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c39b8-4547-4247-8a42-c03cf7bd090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark haversine numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090b7cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 14:05:41 WARN Utils: Your hostname, Divyanshs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.208 instead (on interface en0)\n",
      "24/10/28 14:05:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/28 14:05:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/divyansh/.pyenv/versions/3.9.5/envs/.pyspark/lib/python3.9/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf = conf.setAppName(\"<my-app-name>\")\n",
    "# conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040') ## to setup SPARK UI\n",
    "# conf = conf.set('spark.jars', os.environ['GRAPHFRAMES_PATH']) ## graphframes in spark configuration\n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "except ValueError:\n",
    "    # If a SparkContext is already running, get it\n",
    "    sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    " # streaming representation of this variable (jp notebook thingy)\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93353e9a-5993-475a-9040-975ff178a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bakery = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"data/Bakery.csv\") \n",
    "# bakery.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6d9fee-7ad7-4c4f-8ba9-b260f5f07d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-------+----------+-----------+\n",
      "|Item     |qty|weekday|Date      |Hour-Period|\n",
      "+---------+---+-------+----------+-----------+\n",
      "|Coffee   |2  |Monday |2016-10-31|8          |\n",
      "|Coffee   |11 |Monday |2016-10-31|9          |\n",
      "|Coffee   |10 |Monday |2016-10-31|10         |\n",
      "|Coffee   |13 |Monday |2016-10-31|11         |\n",
      "|Pastry   |1  |Monday |2016-11-07|8          |\n",
      "|Pastry   |3  |Monday |2016-11-07|9          |\n",
      "|Coffee   |7  |Monday |2016-11-07|10         |\n",
      "|Coffee   |10 |Monday |2016-11-07|11         |\n",
      "|Coffee   |1  |Monday |2016-11-14|7          |\n",
      "|Medialuna|2  |Monday |2016-11-14|8          |\n",
      "|Coffee   |5  |Monday |2016-11-14|9          |\n",
      "|Coffee   |5  |Monday |2016-11-14|10         |\n",
      "|Bread    |5  |Monday |2016-11-14|11         |\n",
      "|Coffee   |1  |Monday |2016-11-21|7          |\n",
      "|Coffee   |2  |Monday |2016-11-21|8          |\n",
      "|Coffee   |8  |Monday |2016-11-21|9          |\n",
      "|Coffee   |4  |Monday |2016-11-21|10         |\n",
      "|Coffee   |4  |Monday |2016-11-21|11         |\n",
      "|Coffee   |1  |Monday |2016-11-28|7          |\n",
      "|Coffee   |1  |Monday |2016-11-28|8          |\n",
      "+---------+---+-------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filteredBakery = bakery \\\n",
    "    .filter(F.dayofweek(\"Date\") == 2) \\\n",
    "    .withColumn(\"Hour-Period\", F.hour(\"Time\")) \\\n",
    "    .filter((F.col(\"Hour-Period\") >= 7) & (F.col(\"Hour-Period\") <= 11)) \\\n",
    "    .withColumn(\"weekday\", F.lit(\"Monday\"))\n",
    "\n",
    "itemCounts = filteredBakery \\\n",
    "    .groupBy(\"Date\", \"Hour-Period\", \"Item\", \"weekday\") \\\n",
    "    .agg(F.count(\"Item\").alias(\"qty\"))\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Date\", \"Hour-Period\").orderBy(F.desc(\"qty\"))\n",
    "\n",
    "topItems = itemCounts \\\n",
    "    .withColumn(\"rank\", F.row_number().over(windowSpec)) \\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .orderBy(\"Date\", \"Hour-Period\")\n",
    "\n",
    "topItems.select(\"Item\", \"qty\", \"weekday\", \"Date\", \"Hour-Period\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37169ab-3005-41ac-ae12-8b13fa9b5cfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------------+\n",
      "|DayType|DayPart  |Top2Items      |\n",
      "+-------+---------+---------------+\n",
      "|Weekday|Breakfast|[Coffee, Bread]|\n",
      "|Weekday|Dinner   |[Coffee, Bread]|\n",
      "|Weekday|Lunch    |[Coffee, Bread]|\n",
      "|Weekend|Breakfast|[Coffee, Bread]|\n",
      "|Weekend|Dinner   |[Coffee, Bread]|\n",
      "|Weekend|Lunch    |[Coffee, Bread]|\n",
      "+-------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformedBakery = bakery \\\n",
    "    .withColumn(\"DayPart\", \n",
    "        F.when((F.hour(\"Time\") >= 6) & (F.hour(\"Time\") < 11), \"Breakfast\")\n",
    "         .when((F.hour(\"Time\") >= 11) & (F.hour(\"Time\") < 16), \"Lunch\")\n",
    "         .otherwise(\"Dinner\")\n",
    "    )\n",
    "\n",
    "transformedBakery = transformedBakery \\\n",
    "    .withColumn(\"DayType\", \n",
    "        F.when((F.dayofweek(\"Date\") == 7) | (F.dayofweek(\"Date\") == 1), \"Weekend\")\n",
    "         .otherwise(\"Weekday\")\n",
    "    )\n",
    "\n",
    "transformedBakery = transformedBakery \\\n",
    "    .groupBy(\"DayType\", \"DayPart\", \"Item\") \\\n",
    "    .agg(F.count(\"Item\").alias(\"qty\"))\n",
    "\n",
    "windowSpec = Window.partitionBy(\"DayType\", \"DayPart\").orderBy(F.desc(\"qty\"))\n",
    "\n",
    "rankedBakery = transformedBakery.withColumn(\"Rank\", F.row_number().over(windowSpec))\n",
    "\n",
    "top2Bakery = rankedBakery.filter(F.col(\"Rank\") <= 2)\n",
    "\n",
    "finalResult = top2Bakery \\\n",
    "    .groupBy(\"DayType\", \"DayPart\") \\\n",
    "    .agg(F.collect_list(\"Item\").alias(\"Top2Items\"))\n",
    "\n",
    "finalResult.select(\"DayType\", \"DayPart\", \"Top2Items\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c60a6c0d-f4f8-4c14-a1ea-470b940b980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .json(\"data/Restaurants_in_Durham_County_NC.json\") \n",
    "# restaurants.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b78142-69d0-461e-ba8a-1cc5b7a05aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|rpt_area_desc        |count|\n",
      "+---------------------+-----+\n",
      "|Bed&Breakfast Home   |4    |\n",
      "|Summer Camps         |4    |\n",
      "|Institutions         |30   |\n",
      "|Local Confinement    |2    |\n",
      "|Mobile Food          |147  |\n",
      "|School Buildings     |89   |\n",
      "|Summer Food          |242  |\n",
      "|Swimming Pools       |420  |\n",
      "|Day Care             |173  |\n",
      "|Tattoo Establishments|36   |\n",
      "|Residential Care     |154  |\n",
      "|Bed&Breakfast Inn    |2    |\n",
      "|Adult Day Care       |5    |\n",
      "|Lodging              |62   |\n",
      "|Food Service         |1093 |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_by_rpt = restaurants.groupBy(\"fields.rpt_area_desc\").count()\n",
    "count_by_rpt.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca4f92b-7212-4c50-b5e4-7d24f58647a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|              Region|  PercentageChange|\n",
      "+--------------------+------------------+\n",
      "|United Arab Emirates| 76.27926078028749|\n",
      "|          Montserrat|-63.18732525629077|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 14:05:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: --, 1990, 2000\n",
      " Schema: _c0, 1990, 2000\n",
      "Expected: _c0 but found: --\n",
      "CSV file: file:///Users/divyansh/Programming/bid-data/hw2/data/populationbycountry19802010millions.csv\n",
      "24/10/28 14:05:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: --, 1990, 2000\n",
      " Schema: _c0, 1990, 2000\n",
      "Expected: _c0 but found: --\n",
      "CSV file: file:///Users/divyansh/Programming/bid-data/hw2/data/populationbycountry19802010millions.csv\n"
     ]
    }
   ],
   "source": [
    "population = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"nullValue\", \"NA\") \\\n",
    "  .option(\"nullValue\", \"--\") \\\n",
    "  .csv(\"data/populationbycountry19802010millions.csv\") \n",
    "\n",
    "population = population.withColumnRenamed(\"_c0\", \"Region\")\\\n",
    "    .replace(\"NA\", None) \\\n",
    "    .replace(\"--\", None)\n",
    "\n",
    "selected_columns = [\"Region\", \"1990\", \"2000\"]\n",
    "\n",
    "sample_population = population.select(*selected_columns)\\\n",
    "    .filter(population.Region != 'World')\\\n",
    "    .na.drop(subset=[\"1990\", \"2000\"])\n",
    "\n",
    "for column in [\"1990\", \"2000\"]:\n",
    "    sample_population = sample_population.withColumn(\n",
    "        column, \n",
    "        F.when(F.col(column).cast(\"double\").isNotNull(), F.col(column).cast(\"double\")).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "percentage_change = sample_population.withColumn(\n",
    "    \"PercentageChange\",\n",
    "    F.when(F.col(\"1990\") == 0, 0)  # If 1990 population is 0, return 0\n",
    "     .otherwise(((F.col(\"2000\") - F.col(\"1990\")) / F.col(\"1990\")) * 100)  # Else, calculate the percentage change\n",
    ")\n",
    "\n",
    "max_increase = percentage_change.orderBy(F.desc(\"PercentageChange\")).limit(1)\n",
    "max_decrease = percentage_change.orderBy(F.asc(\"PercentageChange\")).limit(1)\n",
    "\n",
    "biggest_delta = max_increase.union(max_decrease)\n",
    "\n",
    "biggest_delta.select(\"Region\", \"PercentageChange\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea8d861f-6a5f-402e-9da8-3c01d4e6e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = spark.read.text(\"data/hw1text\")\n",
    "\n",
    "processed_df = text_df.select(\n",
    "F.regexp_replace(\n",
    "        F.regexp_replace(F.lower(F.col(\"value\")), \"[^0-9a-z]\", \" \"),\n",
    "        \"\\\\s+\", \" \"\n",
    "    ).alias(\"cleaned_text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08dff2c-1a4b-42b7-88ec-533a8ef1d390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|word| count|\n",
      "+----+------+\n",
      "| the|163547|\n",
      "|  to| 89046|\n",
      "|   p| 78664|\n",
      "|  of| 75568|\n",
      "| and| 72730|\n",
      "|  in| 56782|\n",
      "|   a| 53198|\n",
      "| for| 29770|\n",
      "|that| 28852|\n",
      "|  is| 27601|\n",
      "|  on| 24485|\n",
      "|   s| 23615|\n",
      "|with| 19575|\n",
      "| are| 19417|\n",
      "|  it| 18231|\n",
      "|  be| 17998|\n",
      "|  as| 17796|\n",
      "|have| 16188|\n",
      "|  at| 15965|\n",
      "|  we| 15754|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "words_df = processed_df.select(\n",
    "    F.explode(F.split(F.col(\"cleaned_text\"), \" \")).alias(\"word\")\n",
    ")\n",
    "\n",
    "word_count_df = words_df.groupBy(\"word\").count()\n",
    "\n",
    "word_count_df.orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85a0f9ff-836a-4c82-8b6d-54b7ccb62bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|bigram  |count|\n",
      "+--------+-----+\n",
      "|of the  |17484|\n",
      "|in the  |12808|\n",
      "|p the   |10363|\n",
      "|covid 19|8762 |\n",
      "|to the  |8372 |\n",
      "|for the |5588 |\n",
      "|n t     |5393 |\n",
      "|on the  |5032 |\n",
      "|to be   |4581 |\n",
      "|will be |4177 |\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "tokenized_df = tokenizer.transform(processed_df)\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "ngram_df = ngram.transform(tokenized_df)\n",
    "\n",
    "bigrams_df = ngram_df.select(F.explode(F.col(\"bigrams\")).alias(\"bigram\"))\n",
    "bigram_count_df = bigrams_df.groupBy(\"bigram\").count()\n",
    "\n",
    "top_bigrams = bigram_count_df.orderBy(F.desc(\"count\")).limit(10)\n",
    "\n",
    "top_bigrams.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4732b1bb-d783-47cc-8681-bcea3596d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreclosure = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .json(\"data/durham-nc-foreclosure-2006-2016.json\")\n",
    "\n",
    "# foreclosure.printSchema()\n",
    "# restaurants.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce746a65-2efe-4c7e-8638-1357f2a0d63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datasetid</th>\n",
       "      <th>fields</th>\n",
       "      <th>geometry</th>\n",
       "      <th>record_timestamp</th>\n",
       "      <th>recordid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>restaurants-data</td>\n",
       "      <td>(None, Full-Service Restaurant, [35.9932826, -78.8981331], None, 85098, 4, 2011-01-10, 310 E. MAIN ST., None, DURHAM, OLD HAVANA SANDWICH SHOP, (919) 667-9525, NC, 27701, 4, Food Service, 44, 3 - Municipal/Community, NO, ACTIVE, FOOD, 1 - Restaurant, 5 - Municipal/Community)</td>\n",
       "      <td>([-78.8981331, 35.9932826], Point)</td>\n",
       "      <td>2017-07-13T09:15:31-04:00</td>\n",
       "      <td>2ea5f1269a5c78304997d6fa69de63c7ab18e08e</td>\n",
       "      <td>35.993283</td>\n",
       "      <td>-78.898133</td>\n",
       "      <td>0.125822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          datasetid                                                                                                                                                                                                                                                                               fields                            geometry           record_timestamp                                  recordid   latitude  longitude  distance\n",
       "0  restaurants-data  (None, Full-Service Restaurant, [35.9932826, -78.8981331], None, 85098, 4, 2011-01-10, 310 E. MAIN ST., None, DURHAM, OLD HAVANA SANDWICH SHOP, (919) 667-9525, NC, 27701, 4, Food Service, 44, 3 - Municipal/Community, NO, ACTIVE, FOOD, 1 - Restaurant, 5 - Municipal/Community)  ([-78.8981331, 35.9932826], Point)  2017-07-13T09:15:31-04:00  2ea5f1269a5c78304997d6fa69de63c7ab18e08e  35.993283 -78.898133  0.125822"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haversine import haversine, Unit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "target_location = (35.994914, -78.897133)\n",
    "\n",
    "filtered_df = restaurants.filter(\n",
    "    (F.col(\"fields.status\") == \"ACTIVE\") &\n",
    "    (F.col(\"fields.rpt_area_desc\") == \"Food Service\")\n",
    ")\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    point1 = (lat1, lon1)\n",
    "    point2 = (lat2, lon2)\n",
    "    return haversine(point1, point2, unit=Unit.MILES)\n",
    "\n",
    "haversine_udf = F.udf(haversine_distance, DoubleType())\n",
    "\n",
    "restaurants_with_coordinates = filtered_df.withColumn(\"latitude\", F.col(\"geometry.coordinates\")[1]) \\\n",
    "                                          .withColumn(\"longitude\", F.col(\"geometry.coordinates\")[0])\n",
    "\n",
    "restaurants_with_coordinates = restaurants_with_coordinates.filter(\n",
    "    (F.col(\"latitude\").isNotNull()) &\n",
    "    (F.col(\"longitude\").isNotNull())\n",
    ")\n",
    "\n",
    "restaurants_with_distances = restaurants_with_coordinates.withColumn(\n",
    "    \"distance\",\n",
    "    haversine_udf(\n",
    "        F.col(\"latitude\"),\n",
    "        F.col(\"longitude\"),\n",
    "        F.lit(target_location[0]),\n",
    "        F.lit(target_location[1])\n",
    "    )\n",
    ")\n",
    "\n",
    "closest_restaurant = restaurants_with_distances.orderBy(\"distance\").limit(1)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "closest_restaurant.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "074098ad-6c64-48c3-879e-70461467366b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreclosures_df = foreclosure.withColumn(\"latitude\", F.col(\"geometry.coordinates\")[1])\\\n",
    "    .withColumn(\"longitude\", F.col(\"geometry.coordinates\")[0])\n",
    "\n",
    "\n",
    "foreclosures_cleaned = foreclosures_df.filter(\n",
    "    (F.col(\"latitude\").isNotNull()) & \n",
    "    (F.col(\"longitude\").isNotNull())\n",
    ")\n",
    "\n",
    "latitude, longitude = closest_restaurant.select(\"latitude\", \"longitude\").first()\n",
    "\n",
    "foreclosures_with_distance = foreclosures_cleaned.withColumn(\n",
    "    \"distance\",\n",
    "    haversine_udf(\n",
    "        F.col(\"latitude\"),\n",
    "        F.col(\"longitude\"),\n",
    "        F.lit(latitude),\n",
    "        F.lit(longitude)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter for foreclosures within 1 mile\n",
    "foreclosures_within_one_mile = foreclosures_with_distance.filter(F.col(\"distance\") <= 1)\n",
    "\n",
    "# Show the results\n",
    "foreclosures_within_one_mile.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
